{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "222af72b-3f7e-495f-a394-251f95e6b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, csv, statistics, math, numpy as np, exifread, geopandas as gpd, pandas as pd, cameratransform as ct\n",
    "from osgeo import gdal, osr\n",
    "from shapely.geometry import Point, Polygon\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "# set the base directory where everything lives\n",
    "base_dir = r\"C:\\Users\\gdlarsen\\Documents\\UAS_survey-JHI_WE_S2\\F1\"\n",
    "out_dir = base_dir\n",
    "\n",
    "# set the subdirectories where the images (img) and flight log (flog) live\n",
    "img_dir = f\"{base_dir}\\\\OUTPUTS\"\n",
    "flog_dir = f\"{base_dir}\\\\FLIGHT RECORD\"\n",
    "\n",
    "# set the subdirectories where the georeferenced images will be output and where the footprints feature will be output\n",
    "imgout_dir = f\"{out_dir}\\\\Georeferenced\"\n",
    "shpout_dir = f\"{out_dir}\\\\Shapefiles\"\n",
    "\n",
    "# create output directories, if necessary\n",
    "os.makedirs(shpout_dir, exist_ok = True)\n",
    "os.makedirs(imgout_dir, exist_ok = True)\n",
    "\n",
    "# Look up the sensor width (mm) of the camera, if you can find it: https://www.dxomark.com/\n",
    "# If you can't find it, use 'unknown', for example: sensor_width = 'unknown'\n",
    "\n",
    "#This is the sensor width for the Sony a6100\n",
    "sensor_width = 23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1e0b695-0605-4e1b-80cf-34d927a30755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function scrapes chunks of metadata for the Sony a6100\n",
    "# it's the most I could find using native python methods\n",
    "# (i.e. not calling ExifTool or anything else in the background)\n",
    "# could more simply and robustly recode this section to just call and import from exiftool\n",
    "# might be improved with more reading about EXIF structure and targeted tag IDs \n",
    "# https://exiftool.org/TagNames/EXIF.html\n",
    "\n",
    "def scrape_metadata(img):\n",
    "    metadata_package = {}\n",
    "    with Image.open(img) as im:\n",
    "        exif = im.getexif()\n",
    "        exif_data = exif.get_ifd(0x8769)\n",
    "        metadata_chunk1={}\n",
    "        for tag_id in exif_data:\n",
    "            tag = ExifTags.TAGS.get(tag_id, tag_id)\n",
    "            content = [exif_data.get(tag_id)]\n",
    "            if type(content).__name__ == 'IFDRational':\n",
    "                content = float(content)\n",
    "            metadata_chunk1[tag] = content\n",
    "        FunctionExif = \"b'0231'\"\n",
    "        if str(metadata_chunk1['ExifVersion'][0]) != str(FunctionExif):\n",
    "            print(f\"Warning: this function was designed for metadata produced in {FunctionExif} format, not {metadata_chunk1['ExifVersion'][0]}. Proceed with caution, metadata could be broken.\")\n",
    "        del metadata_chunk1['MakerNote'], metadata_chunk1['FlashPixVersion'], metadata_chunk1['FileSource'], metadata_chunk1['SceneType'], metadata_chunk1['ComponentsConfiguration']\n",
    "        if sensor_width == 'unknown':\n",
    "            metadata_chunk1['SensorWidth'] = [float(36*metadata_chunk1['FocalLength'][0]/metadata_chunk1['FocalLengthIn35mmFilm'][0])]\n",
    "        else:\n",
    "            metadata_chunk1['SensorWidth'] = [sensor_width]\n",
    "        metadata_chunk1['FieldOfView'] = [math.degrees(2*math.atan(metadata_chunk1['SensorWidth'][0]/(2*metadata_chunk1['FocalLength'][0])))]\n",
    "        metadata_chunk1['AspectRatio'] = [metadata_chunk1['ExifImageWidth'][0]/metadata_chunk1['ExifImageHeight'][0]]\n",
    "        metadata_package.update(metadata_chunk1)\n",
    "        \n",
    "        metadata_chunk2 = {}\n",
    "        for tag_id in ExifTags.IFD:\n",
    "            ifd = exif.get_ifd(tag_id)\n",
    "            if tag_id == ExifTags.IFD.GPSInfo:\n",
    "                resolve = ExifTags.GPSTAGS\n",
    "                for k, v in ifd.items():\n",
    "                    tag = resolve.get(k, k)\n",
    "                    metadata_chunk2[tag] = [v]\n",
    "        del metadata_chunk2['GPSVersionID'], metadata_chunk2['GPSAltitudeRef']\n",
    "        metadata_chunk2['GPSLongitudeDD'] = [(float(metadata_chunk2['GPSLongitude'][0][0]) + float(metadata_chunk2['GPSLongitude'][0][1])/60 + float(metadata_chunk2['GPSLongitude'][0][2])/(60*60))*(-1 if metadata_chunk2['GPSLongitudeRef'][0] == 'W' else 1)]\n",
    "        metadata_chunk2['GPSLatitudeDD'] = [(float(metadata_chunk2['GPSLatitude'][0][0]) + float(metadata_chunk2['GPSLatitude'][0][1])/60 + float(metadata_chunk2['GPSLatitude'][0][2])/(60*60))*(-1 if metadata_chunk2['GPSLatitudeRef'][0] == 'S' else 1)]\n",
    "        metadata_package.update(metadata_chunk2)\n",
    "\n",
    "        xmpdata = im.getxmp()['xmpmeta']['RDF']['Description']\n",
    "        metadata_chunk3 = xmpdata\n",
    "        metadata_package.update(metadata_chunk3)\n",
    "\n",
    "        FunctionCamera = 'ILCE-6100 v1.00'\n",
    "        if str(metadata_chunk3['CreatorTool']) != str(FunctionCamera):\n",
    "            print(f\"Warning: this function was designed for metadata produced by {FunctionCamera}, not {metadata_chunk3['CreatorTool']}. Proceed with caution, metadata could be broken.\")\n",
    "    return metadata_package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d33a9f7f-2ec2-448b-8565-7a1b95dcfd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created\n"
     ]
    }
   ],
   "source": [
    "# This section reads the jpgs and extracts the metadata into a table, using the previous function\n",
    "img_list = glob.glob(f\"{base_dir}//*OUTPUT//*.jpg\", recursive=True)\n",
    "\n",
    "# img_list = img_list[355:365] # for demo\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for img in img_list:\n",
    "    image_name = img.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "    metadata = scrape_metadata(img)\n",
    "    sensor_size = float(metadata['SensorWidth'][0]), float(metadata['SensorWidth'][0]/metadata['AspectRatio'][0])\n",
    "    metadict = {'ImageName': [image_name], 'ImagePath': [img]}|metadata|{'SensorSize': [sensor_size]}\n",
    "    df = pd.concat([df, pd.DataFrame.from_dict(metadict)])\n",
    "df = df.reset_index()\n",
    "\n",
    "# This section corrects the camera time for timezone/offsets\n",
    "from datetime import datetime as dt\n",
    "\n",
    "if df['OffsetTime'].loc[0][0] == \"+\":\n",
    "    offset_subtraction = True\n",
    "elif df['OffsetTime'].loc[0][0] == \"-\":\n",
    "    offset_subtraction = True\n",
    "else:\n",
    "    print(f\"might need to code for an unsigned offset, value {df['OffsetTime'].loc[0][0]}\")\n",
    "\n",
    "# This code assumes that the time offset is consistent across the flight (reasonable assumption)\n",
    "# If it varies we can recode a vectorized solution.\n",
    "hoffset, moffset = df['OffsetTime'].loc[0][1:].split(\":\")\n",
    "time_mod = pd.to_datetime(df['DateTimeOriginal'], format = \"%Y:%m:%d %H:%M:%S\")\n",
    "if offset_subtraction:\n",
    "    time_mod = time_mod - (pd.offsets.Hour(int(hoffset)) + pd.offsets.Minute(int(moffset)))\n",
    "else:\n",
    "    time_mod = time_mod + (pd.offsets.Hour(int(hoffset)) + pd.offsets.Minute(int(moffset)))\n",
    "df['DateTime(UTC)'] = time_mod\n",
    "print(\"Table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eb69d8d-798a-4b47-bf73-671755f4997d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading log file C:\\Users\\gdlarsen\\Documents\\UAS_survey-JHI_WE_S2\\F1//FLIGHT RECORD\\JHI_120624_ext_S2_F1 Flight 01.csv\n",
      "Drone altitude merged with table\n"
     ]
    }
   ],
   "source": [
    "# Altitude is often unreliable metadata, whether from GPS or barometer.\n",
    "# In this case, from inspecting photos, it's clear that the drone\n",
    "# was closer to the water than GPS measurements represent\n",
    "# so we're merging the drone flight log (based on time) so we can pull the drone altitude\n",
    "\n",
    "# read in the CSV flight log\n",
    "csv_list = glob.glob(f\"{base_dir}//FLIGHT RECORD//*.csv\", recursive=True)\n",
    "ulog_list = glob.glob(f\"{base_dir}//FLIGHT RECORD//*.ulg\", recursive=True)\n",
    "print(f\"Reading log file {csv_list[0]}\")\n",
    "csv_df = pd.read_csv(csv_list[0])\n",
    "\n",
    "altitude_offset = []\n",
    "# format and merge the CSV flight log into our dataframe\n",
    "csv_df = csv_df.rename(columns={'time(epoch)':'DateTime(Epoch)', 'time(UTC)':'DateTime(UTC)', 'lat':'DroneLatitude', 'lon':'DroneLongitude', 'alt':'DroneAltitude'})\n",
    "csv_df['DateTime(UTC)']=pd.to_datetime(csv_df['DateTime(UTC)'], format = \"%Y-%m-%d %H:%M:%S\")\n",
    "new_df = pd.merge(df, csv_df, on = 'DateTime(UTC)')\n",
    "if len(df) != len(new_df):\n",
    "    altitude_offset = statistics.mode(csv_df['DroneAltitude']) - statistics.mode([float(i) for i in df['GPSAltitude']])\n",
    "    print(f\"merged table is a different length than original table, defaulting to GPSAltitude with an offset of {altitude_offset} m\")\n",
    "    df['DroneAltitude'] = \"NA\"\n",
    "    print(\"Drone altitude column populated with NAs\")\n",
    "    # this might happen if not all camera timestamps occur among drone timestamps\n",
    "else:\n",
    "    df = new_df\n",
    "    print(\"Drone altitude merged with table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f58ba810-35b9-46fe-92e7-a5fd9160ba82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCPs ready\n"
     ]
    }
   ],
   "source": [
    "# This section reads uses the metadata to generate GCPs for image projection\n",
    "\n",
    "df2 = pd.DataFrame()\n",
    "\n",
    "for index, metadata in df.iterrows():\n",
    "    # queue up important parameters from the metadata\n",
    "    f = metadata['FocalLength']\n",
    "    sensor_size = metadata['SensorWidth'], metadata['SensorWidth']/metadata['AspectRatio']\n",
    "    image_size = metadata['ExifImageWidth'], metadata['ExifImageHeight']\n",
    "    lat, lon = metadata['GPSLatitudeDD'], metadata['GPSLongitudeDD']\n",
    "    if not altitude_offset:\n",
    "        alt = float(metadata['DroneAltitude'])\n",
    "    else:\n",
    "        alt = float(metadata['GPSAltitude']) + altitude_offset\n",
    "    yaw, pitch, roll = float(metadata['Yaw']), float(metadata['Pitch']), float(metadata['Roll'])\n",
    "\n",
    "    # use cameratransform package to project image based on parameters\n",
    "    cam = ct.Camera(ct.RectilinearProjection(focallength_mm = f, sensor = sensor_size, image = image_size),\n",
    "                    ct.SpatialOrientation(elevation_m = alt, tilt_deg = pitch, roll_deg = roll, heading_deg = yaw, \n",
    "                                        pos_x_m = 0, pos_y_m = 0))\n",
    "    \n",
    "    # use cameratransform package to assign spatial values to image locations\n",
    "    cam.setGPSpos(lat, lon, alt)\n",
    "\n",
    "    img_edgepoints = ([0, 0], # top left\n",
    "                      [img_w-1, 0], # top right\n",
    "                      [img_w-1, img_h-1], # bottom right\n",
    "                      [0, img_h-1], # bottom left\n",
    "                 \n",
    "                      [(img_w-1)/2, 0], # top midpoint\n",
    "                      [img_w-1, (img_h-1)/2], # right midpoint\n",
    "                      [(img_w-1)/2, img_h-1], # bottom midpoint\n",
    "                      [0, (img_h-1)/2] # left midpoint\n",
    "                     )\n",
    "    GCP_list = [cam.gpsFromImage(i).tolist() + i for i in img_edgepoints]\n",
    "\n",
    "    metadict = {'ImageName': metadata['ImageName'], 'GCPList': [GCP_list], 'geometry': [Polygon([i[0:2][::-1] for i in GCP_list][0:4])]}\n",
    "    df2 = pd.concat([df2, pd.DataFrame.from_dict(metadict)])\n",
    "\n",
    "gdf = gpd.GeoDataFrame(pd.merge(df, df2, on='ImageName'), crs=\"EPSG:4326\")\n",
    "print(\"GCPs ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1bec5a92-fe6f-4457-acda-5e748668392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[58.85624240154075, -137.0952535893452], [58.85580910836961, -137.09415454431812], [58.855419167501985, -137.09470616860912], [58.85586401149918, -137.0958344306931], [58.85602576220367, -137.0947040788201], [58.8556166991308, -137.0944267352741], [58.85564159714027, -137.09527031228575], [58.85605569174655, -137.09554019720008]]\n",
      "[[-137.0952535893452, 58.85624240154075], [-137.09415454431812, 58.85580910836961], [-137.09470616860912, 58.855419167501985], [-137.0958344306931, 58.85586401149918]]\n"
     ]
    }
   ],
   "source": [
    "print([i[0:2] for i in GCP_list])\n",
    "print([i[::-1] for i in coords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50459c82-e028-4262-bd6b-2c3abc5493f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdlarsen\\AppData\\Local\\Temp\\1\\ipykernel_43036\\1507117850.py:9: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  gdf.to_file(out_path)\n",
      "C:\\Users\\gdlarsen\\AppData\\Local\\miniforge3\\envs\\spatial\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Field DateTime(U create as date field, though DateTime requested.\n",
      "  ogr_write(\n",
      "C:\\Users\\gdlarsen\\AppData\\Local\\miniforge3\\envs\\spatial\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value '[[58.86442609712705, -137.06432625188143, 0.0, 0, 0], [58.864872686516996, -137.0654096120216, 0.0, 5999, 0], [58.86524247056331, -137.06483158403734, 0.0, 5999, 3999], [58.864799870942036, -137.06375783177276, 0.0, 0, 3999], [58.86464939891545, -137.06486794291536, 0.0, 2999.5, 0], [58.86505840533121, -137.06511930767905, 0.0, 5999, 1999.5], [58.865021177720244, -137.064294718675, 0.0, 2999.5, 3999], [58.86461381977653, -137.06404077287345, 0.0, 0, 1999.5]]' of field GCPList has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "# This section outputs the geodataframe to a shapefile with all metadata as attributes.\n",
    "# Note that attribute field names get truncated to 10 characters\n",
    "\n",
    "# This section suppresses those warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Normalized/laundered field name\")\n",
    "\n",
    "out_path = f\"{shpout_dir}\\\\Image_footprints.shp\"\n",
    "gdf.to_file(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "068f033b-e9b6-4fcf-bf2b-b3f5ed1747c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed file 150 out of 1685\n",
      "Completed file 160 out of 1685\n",
      "Completed file 170 out of 1685\n",
      "Completed file 180 out of 1685\n",
      "Completed file 190 out of 1685\n",
      "Completed file 200 out of 1685\n",
      "Completed file 210 out of 1685\n",
      "Completed file 220 out of 1685\n",
      "Completed file 230 out of 1685\n",
      "Completed file 240 out of 1685\n",
      "Completed file 250 out of 1685\n",
      "Completed file 260 out of 1685\n",
      "Completed file 270 out of 1685\n",
      "Completed file 280 out of 1685\n",
      "Completed file 290 out of 1685\n",
      "Completed file 300 out of 1685\n",
      "Completed file 310 out of 1685\n",
      "Completed file 320 out of 1685\n",
      "Completed file 330 out of 1685\n",
      "Completed file 340 out of 1685\n",
      "Completed file 350 out of 1685\n",
      "Completed file 360 out of 1685\n",
      "Completed file 370 out of 1685\n",
      "Completed file 380 out of 1685\n",
      "Completed file 390 out of 1685\n",
      "Completed file 400 out of 1685\n",
      "Completed file 410 out of 1685\n",
      "Completed file 420 out of 1685\n",
      "Completed file 430 out of 1685\n",
      "Completed file 440 out of 1685\n",
      "Completed file 450 out of 1685\n",
      "Completed file 460 out of 1685\n",
      "Completed file 470 out of 1685\n",
      "Completed file 480 out of 1685\n",
      "Completed file 490 out of 1685\n",
      "Completed file 500 out of 1685\n",
      "Completed file 510 out of 1685\n",
      "Completed file 520 out of 1685\n",
      "Completed file 530 out of 1685\n",
      "Completed file 540 out of 1685\n",
      "Completed file 550 out of 1685\n",
      "Completed file 560 out of 1685\n",
      "Completed file 570 out of 1685\n",
      "Completed file 580 out of 1685\n",
      "Completed file 590 out of 1685\n",
      "Completed file 600 out of 1685\n",
      "Completed file 610 out of 1685\n",
      "Completed file 620 out of 1685\n",
      "Completed file 630 out of 1685\n",
      "Completed file 640 out of 1685\n",
      "Completed file 650 out of 1685\n",
      "Completed file 660 out of 1685\n",
      "Completed file 670 out of 1685\n",
      "Completed file 680 out of 1685\n",
      "Completed file 690 out of 1685\n",
      "Completed file 700 out of 1685\n",
      "Completed file 710 out of 1685\n",
      "Completed file 720 out of 1685\n",
      "Completed file 730 out of 1685\n",
      "Completed file 740 out of 1685\n",
      "Completed file 750 out of 1685\n",
      "Completed file 760 out of 1685\n",
      "Completed file 770 out of 1685\n",
      "Completed file 780 out of 1685\n",
      "Completed file 790 out of 1685\n",
      "Completed file 800 out of 1685\n",
      "Completed file 810 out of 1685\n",
      "Completed file 820 out of 1685\n",
      "Completed file 830 out of 1685\n",
      "Completed file 840 out of 1685\n",
      "Completed file 850 out of 1685\n",
      "Completed file 860 out of 1685\n",
      "Completed file 870 out of 1685\n",
      "Completed file 880 out of 1685\n",
      "Completed file 890 out of 1685\n",
      "Completed file 900 out of 1685\n",
      "Completed file 910 out of 1685\n",
      "Completed file 920 out of 1685\n",
      "Completed file 930 out of 1685\n",
      "Completed file 940 out of 1685\n",
      "Completed file 950 out of 1685\n",
      "Completed file 960 out of 1685\n",
      "Completed file 970 out of 1685\n",
      "Completed file 980 out of 1685\n",
      "Completed file 990 out of 1685\n",
      "Completed file 1000 out of 1685\n",
      "Completed file 1010 out of 1685\n",
      "Completed file 1020 out of 1685\n",
      "Completed file 1030 out of 1685\n",
      "Completed file 1040 out of 1685\n",
      "Completed file 1050 out of 1685\n",
      "Completed file 1060 out of 1685\n",
      "Completed file 1070 out of 1685\n",
      "Completed file 1080 out of 1685\n",
      "Completed file 1090 out of 1685\n",
      "Completed file 1100 out of 1685\n",
      "Completed file 1110 out of 1685\n",
      "Completed file 1120 out of 1685\n",
      "Completed file 1130 out of 1685\n",
      "Completed file 1140 out of 1685\n",
      "Completed file 1150 out of 1685\n",
      "Completed file 1160 out of 1685\n",
      "Completed file 1170 out of 1685\n",
      "Completed file 1180 out of 1685\n",
      "Completed file 1190 out of 1685\n",
      "Completed file 1200 out of 1685\n",
      "Completed file 1210 out of 1685\n",
      "Completed file 1220 out of 1685\n",
      "Completed file 1230 out of 1685\n",
      "Completed file 1240 out of 1685\n",
      "Completed file 1250 out of 1685\n",
      "Completed file 1260 out of 1685\n",
      "Completed file 1270 out of 1685\n",
      "Completed file 1280 out of 1685\n",
      "Completed file 1290 out of 1685\n",
      "Completed file 1300 out of 1685\n",
      "Completed file 1310 out of 1685\n",
      "Completed file 1320 out of 1685\n",
      "Completed file 1330 out of 1685\n",
      "Completed file 1340 out of 1685\n",
      "Completed file 1350 out of 1685\n",
      "Completed file 1360 out of 1685\n",
      "Completed file 1370 out of 1685\n",
      "Completed file 1380 out of 1685\n",
      "Completed file 1390 out of 1685\n",
      "Completed file 1400 out of 1685\n",
      "Completed file 1410 out of 1685\n",
      "Completed file 1420 out of 1685\n",
      "Completed file 1430 out of 1685\n",
      "Completed file 1440 out of 1685\n",
      "Completed file 1450 out of 1685\n",
      "Completed file 1460 out of 1685\n",
      "Completed file 1470 out of 1685\n",
      "Completed file 1480 out of 1685\n",
      "Completed file 1490 out of 1685\n",
      "Completed file 1500 out of 1685\n",
      "Completed file 1510 out of 1685\n",
      "Completed file 1520 out of 1685\n",
      "Completed file 1530 out of 1685\n",
      "Completed file 1540 out of 1685\n",
      "Completed file 1550 out of 1685\n",
      "Completed file 1560 out of 1685\n",
      "Completed file 1570 out of 1685\n",
      "Completed file 1580 out of 1685\n",
      "Completed file 1590 out of 1685\n",
      "Completed file 1600 out of 1685\n",
      "Completed file 1610 out of 1685\n",
      "Completed file 1620 out of 1685\n",
      "Completed file 1630 out of 1685\n",
      "Completed file 1640 out of 1685\n",
      "Completed file 1650 out of 1685\n",
      "Completed file 1660 out of 1685\n",
      "Completed file 1670 out of 1685\n",
      "Completed file 1680 out of 1685\n",
      "finished processing images\n"
     ]
    }
   ],
   "source": [
    "# This section reads in each image and its GCPs, references and warps the image, then saves it out as a JPEG\n",
    "os.makedirs(imgout_dir, exist_ok = True)\n",
    "\n",
    "# alternative format \"GeoTIFF\" is much larger per file\n",
    "out_format = \"JPEG\"\n",
    "\n",
    "gdal.UseExceptions()\n",
    "\n",
    "for index, record in gdf.iterrows():\n",
    "\n",
    "    # Read in the image file:\n",
    "    ds = gdal.Open(record['ImagePath'])\n",
    "    if ds is None:\n",
    "        print(f\"Could not open image: {record['ImagePath']}\")\n",
    "    # Set spatial reference:\n",
    "    sr = osr.SpatialReference()\n",
    "    sr.ImportFromEPSG(4326)\n",
    "\n",
    "    # Import GCPs - there's got to be a better way to unpack i, but lists get rejected.\n",
    "    gcps = [gdal.GCP(i[0], i[1], i[2], i[3], i[4]) for i in record['GCPList']]\n",
    "\n",
    "    # Apply the GCPs to the open output file then warp it\n",
    "    ds.SetGCPs(gcps, sr.ExportToWkt())\n",
    "\n",
    "    if out_format==\"JPEG\":\n",
    "        kwargs = {'format': 'JPEG', 'polynomialOrder':2, 'srcNodata': '0,0,0', 'dstNodata': 'nodata'}\n",
    "        ds = gdal.Warp(f\"{imgout_dir}\\\\{record['ImageName']}_GeoRef.jpg\", ds, **kwargs)\n",
    "    elif out_format==\"GeoTIFF\":\n",
    "        kwargs = {'format': 'GTiff', 'polynomialOrder':2, 'srcNodata': '0,0,0', 'dstNodata': 'nodata'}\n",
    "        ds = gdal.Warp(f\"{imgout_dir}\\\\{record['ImageName']}_GeoRef.tif\", ds, **kwargs)\n",
    "    else:\n",
    "        print(fr\"format '{out_format}' not currently supported, code it yourself from https://gdal.org/en/latest/drivers/raster/index.html\")\n",
    "        break\n",
    "    # Clear the variable (not sure if necessary, but good form)\n",
    "    ds = None\n",
    "    # counter, just because it can be a long process\n",
    "    if (index+1) % 10 == 0:\n",
    "        print(f\"Completed file {index+1} out of {len(df)+1}\")\n",
    "print(\"finished processing images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd1ce0-6e7b-4041-bd1c-2bdf1243b52b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
